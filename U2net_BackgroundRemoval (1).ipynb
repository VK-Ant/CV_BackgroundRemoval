{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjXCEto_I2Th",
        "outputId": "34e5d352-1b16-4ba9-e5c2-ffb856a49109"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "%cd  /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z88Pl7ctPkVv",
        "outputId": "a6a8e3d9-725d-43fb-8d8e-1905032e0dd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'background_removal_DL' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Nkap23/background_removal_DL.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-t_gILxoPp0L",
        "outputId": "82a731f4-09a1-434a-8f5c-a6bca202aab9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/background_removal_DL\n"
          ]
        }
      ],
      "source": [
        "%cd background_removal_DL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzq3xFO0Qeto",
        "outputId": "0586467d-80c9-450c-c873-4ef17d537aed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['/content/drive/MyDrive/background_removal_DL/test_data/images/input/4.jpeg']\n",
            "...load U2NET---173.6 MB\n",
            "inferencing: 4.jpeg\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from skimage import io, transform\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms#, utils\n",
        "# import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import glob\n",
        "\n",
        "from data_loader import RescaleT\n",
        "from data_loader import ToTensor\n",
        "from data_loader import ToTensorLab\n",
        "from data_loader import SalObjDataset\n",
        "\n",
        "from model import U2NET # full size version 173.6 MB\n",
        "from model import U2NETP # small version u2net 4.7 MB\n",
        "\n",
        "# normalize the predicted SOD probability map\n",
        "def normPRED(d):\n",
        "    ma = torch.max(d)\n",
        "    mi = torch.min(d)\n",
        "\n",
        "    dn = (d-mi)/(ma-mi)\n",
        "\n",
        "    return dn\n",
        "\n",
        "def save_output(image_name,pred,d_dir):\n",
        "\n",
        "    predict = pred\n",
        "    predict = predict.squeeze()\n",
        "    predict_np = predict.cpu().data.numpy()\n",
        "\n",
        "    im = Image.fromarray(predict_np*255).convert('RGB')\n",
        "    img_name = image_name.split(os.sep)[-1]\n",
        "    image = io.imread(image_name)\n",
        "    imo = im.resize((image.shape[1],image.shape[0]),resample=Image.BILINEAR)\n",
        "\n",
        "    pb_np = np.array(imo)\n",
        "\n",
        "    aaa = img_name.split(\".\")\n",
        "    bbb = aaa[0:-1]\n",
        "    imidx = bbb[0]\n",
        "    for i in range(1,len(bbb)):\n",
        "        imidx = imidx + \".\" + bbb[i]\n",
        "\n",
        "    imo.save(d_dir+imidx+'.png')\n",
        "\n",
        "def main():\n",
        "\n",
        "    # --------- 1. get image path and name ---------\n",
        "    model_name='u2net'#u2netp\n",
        "\n",
        "\n",
        "\n",
        "    image_dir=os.path.join(os.getcwd(),'test_data','images','input')\n",
        "    prediction_dir=os.path.join(os.getcwd(),'test_data','images',model_name+'_results'+os.sep)\n",
        "    model_dir = os.path.join(os.getcwd(),'saved_models',model_name,model_name+'.pth')\n",
        "\n",
        "    img_name_list = glob.glob(image_dir + os.sep + '*')\n",
        "    print(img_name_list)\n",
        "\n",
        "    # --------- 2. dataloader ---------\n",
        "    #1. dataloader\n",
        "    test_salobj_dataset = SalObjDataset(img_name_list = img_name_list,\n",
        "                                        lbl_name_list = [],\n",
        "                                        transform=transforms.Compose([RescaleT(320),\n",
        "                                                                      ToTensorLab(flag=0)])\n",
        "                                        )\n",
        "    test_salobj_dataloader = DataLoader(test_salobj_dataset,\n",
        "                                        batch_size=1,\n",
        "                                        shuffle=False,\n",
        "                                        num_workers=1)\n",
        "\n",
        "    # --------- 3. model define ---------\n",
        "    if(model_name=='u2net'):\n",
        "        print(\"...load U2NET---173.6 MB\")\n",
        "        net = U2NET(3,1)\n",
        "    elif(model_name=='u2netp'):\n",
        "        print(\"...load U2NEP---4.7 MB\")\n",
        "        net = U2NETP(3,1)\n",
        "    net.load_state_dict(torch.load(model_dir))\n",
        "    if torch.cuda.is_available():\n",
        "        net.cuda()\n",
        "    net.eval()\n",
        "\n",
        "    # --------- 4. inference for each image ---------\n",
        "    for i_test, data_test in enumerate(test_salobj_dataloader):\n",
        "\n",
        "        print(\"inferencing:\",img_name_list[i_test].split(os.sep)[-1])\n",
        "\n",
        "        inputs_test = data_test['image']\n",
        "        inputs_test = inputs_test.type(torch.FloatTensor)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            inputs_test = Variable(inputs_test.cuda())\n",
        "        else:\n",
        "            inputs_test = Variable(inputs_test)\n",
        "\n",
        "        d1,d2,d3,d4,d5,d6,d7= net(inputs_test)\n",
        "\n",
        "        # normalization\n",
        "        pred = d1[:,0,:,:]\n",
        "        pred = normPRED(pred)\n",
        "\n",
        "        # save results to test_results folder\n",
        "        if not os.path.exists(prediction_dir):\n",
        "            os.makedirs(prediction_dir, exist_ok=True)\n",
        "        save_output(img_name_list[i_test],pred,prediction_dir)\n",
        "\n",
        "        del d1,d2,d3,d4,d5,d6,d7\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9blNVC68Qihy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from skimage import io, transform\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms#, utils\n",
        "# import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import glob\n",
        "\n",
        "from data_loader import RescaleT\n",
        "from data_loader import ToTensor\n",
        "from data_loader import ToTensorLab\n",
        "from data_loader import SalObjDataset\n",
        "\n",
        "from model import U2NET # full size version 173.6 MB\n",
        "from model import U2NETP # small version u2net 4.7 MB\n",
        "\n",
        "# normalize the predicted SOD probability map\n",
        "def normPRED(d):\n",
        "    ma = torch.max(d)\n",
        "    mi = torch.min(d)\n",
        "\n",
        "    dn = (d-mi)/(ma-mi)\n",
        "\n",
        "    return dn\n",
        "\n",
        "def save_output(image_name,pred,d_dir):\n",
        "\n",
        "    predict = pred\n",
        "    predict = predict.squeeze()\n",
        "    predict_np = predict.cpu().data.numpy()\n",
        "\n",
        "    im = Image.fromarray(predict_np*255).convert('RGB')\n",
        "    img_name = image_name.split(os.sep)[-1]\n",
        "    image = io.imread(image_name)\n",
        "    imo = im.resize((image.shape[1],image.shape[0]),resample=Image.BILINEAR)\n",
        "\n",
        "    pb_np = np.array(imo)\n",
        "\n",
        "    aaa = img_name.split(\".\")\n",
        "    bbb = aaa[0:-1]\n",
        "    imidx = bbb[0]\n",
        "    for i in range(1,len(bbb)):\n",
        "        imidx = imidx + \".\" + bbb[i]\n",
        "\n",
        "    imo.save(d_dir+imidx+'.png')\n",
        "\n",
        "def main():\n",
        "\n",
        "    # --------- 1. get image path and name ---------\n",
        "    model_name='u2net'#u2netp\n",
        "\n",
        "\n",
        "\n",
        "    image_dir=os.path.join(os.getcwd(),'test_data','images','input')\n",
        "    prediction_dir=os.path.join(os.getcwd(),'test_data','images',model_name+'_results'+os.sep)\n",
        "    model_dir = os.path.join(os.getcwd(),'saved_models',model_name,model_name+'.pth')\n",
        "\n",
        "    img_name_list = glob.glob(image_dir + os.sep + '*')\n",
        "    print(img_name_list)\n",
        "\n",
        "    # --------- 2. dataloader ---------\n",
        "    #1. dataloader\n",
        "    test_salobj_dataset = SalObjDataset(img_name_list = img_name_list,\n",
        "                                        lbl_name_list = [],\n",
        "                                        transform=transforms.Compose([RescaleT(320),\n",
        "                                                                      ToTensorLab(flag=0)])\n",
        "                                        )\n",
        "    test_salobj_dataloader = DataLoader(test_salobj_dataset,\n",
        "                                        batch_size=1,\n",
        "                                        shuffle=False,\n",
        "                                        num_workers=1)\n",
        "\n",
        "    # --------- 3. model define ---------\n",
        "    if(model_name=='u2net'):\n",
        "        print(\"...load U2NET---173.6 MB\")\n",
        "        net = U2NET(3,1)\n",
        "    elif(model_name=='u2netp'):\n",
        "        print(\"...load U2NEP---4.7 MB\")\n",
        "        net = U2NETP(3,1)\n",
        "    net.load_state_dict(torch.load(model_dir))\n",
        "    if torch.cuda.is_available():\n",
        "        net.cuda()\n",
        "    net.eval()\n",
        "\n",
        "    # --------- 4. inference for each image ---------\n",
        "    for i_test, data_test in enumerate(test_salobj_dataloader):\n",
        "\n",
        "        print(\"inferencing:\",img_name_list[i_test].split(os.sep)[-1])\n",
        "\n",
        "        inputs_test = data_test['image']\n",
        "        inputs_test = inputs_test.type(torch.FloatTensor)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            inputs_test = Variable(inputs_test.cuda())\n",
        "        else:\n",
        "            inputs_test = Variable(inputs_test)\n",
        "\n",
        "        d1,d2,d3,d4,d5,d6,d7= net(inputs_test)\n",
        "\n",
        "        # normalization\n",
        "        pred = d1[:,0,:,:]\n",
        "        pred = normPRED(pred)\n",
        "\n",
        "        # save results to test_results folder\n",
        "        if not os.path.exists(prediction_dir):\n",
        "            os.makedirs(prediction_dir, exist_ok=True)\n",
        "        save_output(img_name_list[i_test],pred,prediction_dir)\n",
        "\n",
        "        del d1,d2,d3,d4,d5,d6,d7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUuMpXiDQAih",
        "outputId": "f2160ecf-a856-4b61-afcd-0cdd11e2d561"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.6.0.66)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Du9Q2AnkPSik",
        "outputId": "6402487d-8f94-4d63-fdc0-2a176c0a44bd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import cv2\n",
        "#u2netresult\n",
        "u2netresult=cv2.imread('/content/drive/MyDrive/background_removal_DL/test_data/images/u2net_results/4.png')\n",
        "#orginalimage (CHANGE FILE EXTENSION HERE - BY DEFAULT: .jpg)\n",
        "original=cv2.imread('/content/drive/MyDrive/background_removal_DL/test_data/images/input/4.jpeg')\n",
        "#subimage\n",
        "subimage=cv2.subtract(u2netresult,original)\n",
        "cv2.imwrite('/content/drive/MyDrive/background_removal_DL/test_data/images/output/output.png',subimage)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1-ZI1GEGP-jl"
      },
      "outputs": [],
      "source": [
        "#subimage\n",
        "subimage=Image.open('/content/drive/MyDrive/background_removal_DL/test_data/images/u2net_results/4.png')\n",
        "#originalimage\n",
        "original=Image.open('/content/drive/MyDrive/background_removal_DL/test_data/images/input/4.jpeg')\n",
        "\n",
        "subimage=subimage.convert(\"RGBA\")\n",
        "original=original.convert(\"RGBA\")\n",
        "\n",
        "subdata=subimage.getdata()\n",
        "ogdata=original.getdata()\n",
        "\n",
        "newdata=[]\n",
        "for i in range(subdata.size[0]*subdata.size[1]):\n",
        "  if subdata[i][0]==0 and subdata[i][1]==0 and subdata[i][2]==0:\n",
        "    newdata.append((255,255,255,0))\n",
        "  else:\n",
        "    newdata.append(ogdata[i])\n",
        "subimage.putdata(newdata)\n",
        "subimage.save('/content/drive/MyDrive/background_removal_DL/test_data/images/output/output_bgrm.png',\"PNG\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r286CAefQyw_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.6.13 ('new')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "0be42fca8771cc3f028a9487a47b8ddbb1364ed11601c578719c355e185271a2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
